{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "import keras.initializers as KI\n",
    "from keras.layers import Dense, Dropout, Input, LeakyReLU\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data, Create Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pickle.load(open('../data/train_preprocessed.pkl','rb'))\n",
    "test = pickle.load(open('../data/test_preprocessed.pkl','rb'))\n",
    "labels = pickle.load(open('../data/labels_preprocessed.pkl','rb'))\n",
    "\n",
    "train, val, labels, val_labels = train_test_split(train, labels, random_state=42,\n",
    "                                                 test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 100000  #this is the maximum vocabulary size; hyperparameter\n",
    "SEQUENCE_LENGTH = 150 #this is the length of each sequence being fed to the GRU/LSTM - hyperparameter\n",
    "EMBEDDING_SIZE = 300 #this depends on the embedding values used; 300 is the Google matrix w2v size\n",
    "\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMicroAccuracy(labels, predictions):\n",
    "    _val_f1 = f1_score(labels, predictions, average='micro')\n",
    "    _val_recall = recall_score(labels, predictions, average='micro')\n",
    "    _val_precision = precision_score(labels, predictions, average='micro')\n",
    "    print (\"[Weighted] F1 Score: %f,  Precision: %f,  Recall: %f\" % (_val_f1, _val_precision, _val_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "#calculat the class weights:  2d array (# of classes (ROWS), 2 * (background and signal))\n",
    "def calculating_class_weights(y_true):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    number_dim = np.shape(y_true)[1]\n",
    "    weights = np.empty([number_dim, 2])\n",
    "    for i in range(number_dim):\n",
    "        weights[i] = compute_class_weight('balanced', [0.,1.], y_true[:, i])\n",
    "    return weights\n",
    "\n",
    "#custom loss function\n",
    "def get_weighted_loss(weights):\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        return K.mean((weights[:,0]**(1-y_true))*(weights[:,1]**(y_true))*K.binary_crossentropy(y_true, y_pred), \n",
    "                      axis=-1)\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Text Data (limited by VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE)\n",
    "tokenizer.fit_on_texts(list(train.doc_text.values) + list(val.doc_text.values))  #add TEST for prediction\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= VOCABULARY_SIZE}\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(train.doc_text.values)\n",
    "list_tokenized_val = tokenizer.texts_to_sequences(val.doc_text.values)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(test.doc_text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112.424839228296\n"
     ]
    }
   ],
   "source": [
    "#to choose the appropriate maxlen, find what \"most\" of the sentences have in terms of words and get close to that\n",
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_train]\n",
    "plt.hist(totalNumWords,bins = np.arange(0,2200,100) )\n",
    "plt.show()\n",
    "print(np.mean(totalNumWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pad = pad_sequences(list_tokenized_train, maxlen=SEQUENCE_LENGTH)\n",
    "val_pad = pad_sequences(list_tokenized_val, maxlen=SEQUENCE_LENGTH)\n",
    "#test_pad = pad_sequences(list_tokenized_test, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-18 21:58:14,064 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-02-18 21:58:14,064 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2019-02-18 21:58:14,080 : INFO : loading Word2VecKeyedVectors object from c:/edx/competition/data/vectors.kv\n",
      "2019-02-18 21:58:15,123 : INFO : loading vectors from c:/edx/competition/data/vectors.kv.vectors.npy with mmap=r\n",
      "2019-02-18 21:58:15,123 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-02-18 21:58:15,123 : INFO : loaded c:/edx/competition/data/vectors.kv\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import gensim\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "#w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../data/vectors.kv', binary=False)  \n",
    "from gensim.test.utils import get_tmpfile\n",
    "fname = get_tmpfile(\"c:/edx/competition/data/vectors.kv\")\n",
    "w2v_model = gensim.models.KeyedVectors.load(fname, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word embeddings: 100000\n",
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "nb_words = min(VOCABULARY_SIZE, len(tokenizer.word_index)) + 1\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_SIZE))\n",
    "missing_words = []\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in w2v_model.vocab:\n",
    "        embedding_matrix[i] = w2v_model.word_vec(word)\n",
    "    else:\n",
    "        missing_words.append(word)\n",
    "\n",
    "print('Total word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) != 0))\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"Red\">Keras </font> Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from importlib import reload\n",
    "#reload(model_zoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     30000300    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 150, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 150, 180)     54180       spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 149, 180)     108180      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 148, 180)     162180      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 147, 180)     216180      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_1 (KMaxPooling)   (None, 540)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_2 (KMaxPooling)   (None, 540)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_3 (KMaxPooling)   (None, 540)          0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "k_max_pooling_4 (KMaxPooling)   (None, 540)          0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 2160)         0           k_max_pooling_1[0][0]            \n",
      "                                                                 k_max_pooling_2[0][0]            \n",
      "                                                                 k_max_pooling_3[0][0]            \n",
      "                                                                 k_max_pooling_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2160)         0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 144)          311184      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 29)           4205        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,856,409\n",
      "Trainable params: 856,109\n",
      "Non-trainable params: 30,000,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class_wt = calculating_class_weights(labels.values)\n",
    "\n",
    "from utils import model_zoo\n",
    "#model = model_zoo.get_dropout_bi_gru(nb_words=nb_words, embedding_dim=EMBEDDING_SIZE,\n",
    "#                         embedding_matrix=embedding_matrix, max_sequence_length=SEQUENCE_LENGTH,\n",
    "#                         out_size=len(labels.columns), loss_fn=get_weighted_loss, class_weights=class_wt,\n",
    "#                         val_data=val_pad, val_labels=val_labels)\n",
    "\n",
    "model = model_zoo.get_kmax_text_cnn(nb_words=nb_words, \n",
    "    embedding_dim=EMBEDDING_SIZE, \n",
    "    embedding_matrix=embedding_matrix, \n",
    "    max_sequence_length=SEQUENCE_LENGTH, \n",
    "    out_size=len(labels.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14928 samples, validate on 3732 samples\n",
      "Epoch 1/50\n",
      "14928/14928 [==============================] - 16s 1ms/step - loss: 0.7957 - acc: 0.8296 - f1: 0.2196 - val_loss: 0.3432 - val_acc: 0.8747 - val_f1: 0.2782\n",
      "Epoch 2/50\n",
      "14928/14928 [==============================] - 13s 868us/step - loss: 0.4017 - acc: 0.8602 - f1: 0.2513 - val_loss: 0.2886 - val_acc: 0.8992 - val_f1: 0.2665\n",
      "Epoch 3/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.3451 - acc: 0.8807 - f1: 0.2689 - val_loss: 0.2815 - val_acc: 0.9000 - val_f1: 0.2766\n",
      "Epoch 4/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.3206 - acc: 0.8879 - f1: 0.2879 - val_loss: 0.2751 - val_acc: 0.9010 - val_f1: 0.2945\n",
      "Epoch 5/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.3060 - acc: 0.8922 - f1: 0.2941 - val_loss: 0.2725 - val_acc: 0.9026 - val_f1: 0.3211\n",
      "Epoch 6/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2956 - acc: 0.8951 - f1: 0.3111 - val_loss: 0.2681 - val_acc: 0.9031 - val_f1: 0.3404\n",
      "Epoch 7/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2859 - acc: 0.8978 - f1: 0.3281 - val_loss: 0.2631 - val_acc: 0.9045 - val_f1: 0.3542\n",
      "Epoch 8/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2793 - acc: 0.8994 - f1: 0.3426 - val_loss: 0.2601 - val_acc: 0.9058 - val_f1: 0.3899\n",
      "Epoch 9/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2729 - acc: 0.9013 - f1: 0.3606 - val_loss: 0.2585 - val_acc: 0.9062 - val_f1: 0.3943\n",
      "Epoch 10/50\n",
      "14928/14928 [==============================] - 13s 867us/step - loss: 0.2682 - acc: 0.9024 - f1: 0.3715 - val_loss: 0.2532 - val_acc: 0.9075 - val_f1: 0.4138\n",
      "Epoch 11/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2633 - acc: 0.9040 - f1: 0.3882 - val_loss: 0.2515 - val_acc: 0.9082 - val_f1: 0.4185\n",
      "Epoch 12/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2598 - acc: 0.9054 - f1: 0.4004 - val_loss: 0.2486 - val_acc: 0.9091 - val_f1: 0.4363\n",
      "Epoch 13/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2563 - acc: 0.9059 - f1: 0.4087 - val_loss: 0.2463 - val_acc: 0.9092 - val_f1: 0.4385\n",
      "Epoch 14/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2534 - acc: 0.9068 - f1: 0.4174 - val_loss: 0.2447 - val_acc: 0.9100 - val_f1: 0.4513\n",
      "Epoch 15/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2511 - acc: 0.9073 - f1: 0.4218 - val_loss: 0.2446 - val_acc: 0.9106 - val_f1: 0.4565\n",
      "Epoch 16/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2480 - acc: 0.9085 - f1: 0.4318 - val_loss: 0.2415 - val_acc: 0.9107 - val_f1: 0.4671\n",
      "Epoch 17/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2457 - acc: 0.9096 - f1: 0.4418 - val_loss: 0.2406 - val_acc: 0.9115 - val_f1: 0.4669\n",
      "Epoch 18/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2440 - acc: 0.9099 - f1: 0.4452 - val_loss: 0.2387 - val_acc: 0.9116 - val_f1: 0.4742\n",
      "Epoch 19/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2424 - acc: 0.9103 - f1: 0.4488 - val_loss: 0.2394 - val_acc: 0.9117 - val_f1: 0.4821\n",
      "Epoch 20/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2403 - acc: 0.9108 - f1: 0.4533 - val_loss: 0.2377 - val_acc: 0.9121 - val_f1: 0.4766\n",
      "Epoch 21/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2393 - acc: 0.9109 - f1: 0.4576 - val_loss: 0.2369 - val_acc: 0.9122 - val_f1: 0.4760\n",
      "Epoch 22/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2381 - acc: 0.9116 - f1: 0.4615 - val_loss: 0.2367 - val_acc: 0.9123 - val_f1: 0.4926\n",
      "Epoch 23/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2359 - acc: 0.9122 - f1: 0.4672 - val_loss: 0.2354 - val_acc: 0.9131 - val_f1: 0.4812\n",
      "Epoch 24/50\n",
      "14928/14928 [==============================] - 13s 872us/step - loss: 0.2351 - acc: 0.9125 - f1: 0.4697 - val_loss: 0.2343 - val_acc: 0.9132 - val_f1: 0.4923\n",
      "Epoch 25/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2332 - acc: 0.9128 - f1: 0.4734 - val_loss: 0.2345 - val_acc: 0.9130 - val_f1: 0.4887\n",
      "Epoch 26/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2324 - acc: 0.9132 - f1: 0.4778 - val_loss: 0.2332 - val_acc: 0.9134 - val_f1: 0.4865\n",
      "Epoch 27/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2316 - acc: 0.9135 - f1: 0.4795 - val_loss: 0.2331 - val_acc: 0.9134 - val_f1: 0.5050\n",
      "Epoch 28/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2300 - acc: 0.9142 - f1: 0.4856 - val_loss: 0.2334 - val_acc: 0.9134 - val_f1: 0.5089\n",
      "Epoch 29/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2286 - acc: 0.9145 - f1: 0.4889 - val_loss: 0.2320 - val_acc: 0.9139 - val_f1: 0.5054\n",
      "Epoch 30/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2279 - acc: 0.9144 - f1: 0.4893 - val_loss: 0.2315 - val_acc: 0.9142 - val_f1: 0.5057\n",
      "Epoch 31/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2261 - acc: 0.9153 - f1: 0.4965 - val_loss: 0.2315 - val_acc: 0.9135 - val_f1: 0.5149\n",
      "Epoch 32/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2253 - acc: 0.9154 - f1: 0.4977 - val_loss: 0.2297 - val_acc: 0.9146 - val_f1: 0.5052\n",
      "Epoch 33/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2251 - acc: 0.9150 - f1: 0.4958 - val_loss: 0.2294 - val_acc: 0.9141 - val_f1: 0.5099\n",
      "Epoch 34/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2237 - acc: 0.9155 - f1: 0.5010 - val_loss: 0.2294 - val_acc: 0.9142 - val_f1: 0.5131\n",
      "Epoch 35/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2224 - acc: 0.9163 - f1: 0.5060 - val_loss: 0.2311 - val_acc: 0.9141 - val_f1: 0.5215\n",
      "Epoch 36/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2223 - acc: 0.9160 - f1: 0.5059 - val_loss: 0.2283 - val_acc: 0.9142 - val_f1: 0.5010\n",
      "Epoch 37/50\n",
      "14928/14928 [==============================] - 13s 870us/step - loss: 0.2209 - acc: 0.9165 - f1: 0.5090 - val_loss: 0.2288 - val_acc: 0.9144 - val_f1: 0.5224\n",
      "Epoch 38/50\n",
      "14928/14928 [==============================] - 13s 872us/step - loss: 0.2199 - acc: 0.9168 - f1: 0.5135 - val_loss: 0.2274 - val_acc: 0.9147 - val_f1: 0.5109\n",
      "Epoch 39/50\n",
      "14928/14928 [==============================] - 13s 869us/step - loss: 0.2188 - acc: 0.9170 - f1: 0.5135 - val_loss: 0.2278 - val_acc: 0.9141 - val_f1: 0.5200\n",
      "Epoch 40/50\n",
      "14928/14928 [==============================] - 13s 872us/step - loss: 0.2188 - acc: 0.9168 - f1: 0.5159 - val_loss: 0.2269 - val_acc: 0.9145 - val_f1: 0.5060\n",
      "Epoch 41/50\n",
      "14928/14928 [==============================] - 13s 874us/step - loss: 0.2171 - acc: 0.9173 - f1: 0.5175 - val_loss: 0.2275 - val_acc: 0.9150 - val_f1: 0.5162\n",
      "Epoch 42/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2161 - acc: 0.9179 - f1: 0.5237 - val_loss: 0.2257 - val_acc: 0.9153 - val_f1: 0.5141\n",
      "Epoch 43/50\n",
      "14928/14928 [==============================] - 13s 871us/step - loss: 0.2153 - acc: 0.9175 - f1: 0.5209 - val_loss: 0.2277 - val_acc: 0.9147 - val_f1: 0.5176\n",
      "Epoch 44/50\n",
      " 6500/14928 [============>.................] - ETA: 6s - loss: 0.2148 - acc: 0.9179 - f1: 0.5213"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4406e825ba1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#class_wt = class_weight.compute_sample_weight('balanced', labels.values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m history = model.fit(train_pad, labels.values, epochs=EPOCHS, \n\u001b[1;32m----> 4\u001b[1;33m                     batch_size=BATCH_SIZE, verbose=1, validation_data=(val_pad, val_labels))\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from sklearn.utils import class_weight\n",
    "#class_wt = class_weight.compute_sample_weight('balanced', labels.values)\n",
    "history = model.fit(train_pad, labels.values, epochs=EPOCHS, \n",
    "                    batch_size=BATCH_SIZE, verbose=1, validation_data=(val_pad, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(val_pad).round().astype(np.uint8)\n",
    "GetMicroAccuracy(val_labels.values, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation','f1'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# show the F1 vs val_f1\n",
    "plt.plot(history.history['f1'])\n",
    "plt.plot(history.history['val_f1'])\n",
    "plt.title('model F1')\n",
    "plt.ylabel('F1 score')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
